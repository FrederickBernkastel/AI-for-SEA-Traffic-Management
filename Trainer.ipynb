{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data must have each day fully populated with 24*4 demand data and sequentially labelled\n",
    "# It is assumed that the earliest entry of the training data starts with midnight, and that null entries represent 0 demand\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import Geohash\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "DATA_PATH = os.getcwd() + \"/Data/training.csv\"\n",
    "\n",
    "# XGBoost params\n",
    "max_depth = 6\n",
    "n_estimators = 1000\n",
    "max_bins = 2048\n",
    "\n",
    "# LGB params\n",
    "lgb_params = {\n",
    "    'lambda_l2':1,\n",
    "    'metric': 'rmse',\n",
    "    'objective': 'regression',\n",
    "    'device_type':'cpu',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves':40,\n",
    "    'min_data_in_leaf':50,\n",
    "    'seed' : 1337,\n",
    "    'max_bin': 2048,\n",
    "    'min_split_gain': 0.001,\n",
    "    'subsample_for_bin': 5000,\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_data(data_path=DATA_PATH):\n",
    "    \"\"\"Parses csv file into numpy array\"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Add latt and long columns\n",
    "    df['latt'] = df['geohash6'].apply(lambda x: Geohash.decode(x)[0])\n",
    "    df['long'] = df['geohash6'].apply(lambda x: Geohash.decode(x)[1])\n",
    "\n",
    "    # Extract unique latt and long values\n",
    "    latt_vals = sorted(df['latt'].unique())\n",
    "    long_vals = sorted(df['long'].unique())\n",
    "\n",
    "    latt_to_idx_dic = {latt:idx for idx,latt in enumerate(latt_vals)}\n",
    "    long_to_idx_dic = {long:idx for idx,long in enumerate(long_vals)}\n",
    "\n",
    "    # Save dic\n",
    "    with open(\"Model/latt_to_idx_dic.pkl\",\"wb\") as f:\n",
    "        pickle.dump(latt_to_idx_dic,f)\n",
    "\n",
    "\n",
    "    with open(\"Model/long_to_idx_dic.pkl\",\"wb\") as f:\n",
    "        pickle.dump(long_to_idx_dic,f)\n",
    "\n",
    "    # Populate raw demand data onto np array of shape (day_time_periods,latt,long)\n",
    "    demand_data = []\n",
    "    for day in sorted(df['day'].unique()):\n",
    "        filtered_day = df[df['day'] == day]\n",
    "        print(\"Day %d\"%(day),end='\\r')\n",
    "        for hour in range(24):\n",
    "            for minute in \"0 15 30 45\".split():\n",
    "                timestamp = \"%d:%s\"%(hour,minute)\n",
    "                a = np.zeros((len(latt_vals),len(long_vals)))\n",
    "                filtered_time = filtered_day[filtered_day['timestamp']==timestamp]\n",
    "\n",
    "                for idx,item in filtered_time[['latt','long','demand']].iterrows():\n",
    "                    latt,long,demand = item\n",
    "                    latt_idx = latt_to_idx_dic[latt]\n",
    "                    long_idx = long_to_idx_dic[long]\n",
    "                    a[latt_idx,long_idx] = demand\n",
    "                demand_data.append(a)\n",
    "    return np.stack(demand_data,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    \"\"\"\n",
    "    Extracts features from numpy array of demand, for training\n",
    "    Numpy array should start with midnight for the first day and contain data for at least 8 days\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Numpy array\n",
    "        3D Numpy array of demand data, with the shape (n_periods, n_latt, n_long), where\n",
    "            n_periods refers to the number of 15-minute periods\n",
    "            n_latt refers to the number of lattitude values\n",
    "            n_long refers to the number of longitude values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Numpy Array\n",
    "        4D Numpy array of features, with the shape (n_features, n_feature_periods, n_latt, n_long), where\n",
    "            n_features refers to the number of features for each data sample\n",
    "            n_feature_periods refers to the number of 15-minute periods with features\n",
    "            n_latt refers to the number of lattitude values\n",
    "            n_long refers to the number of longitude values\n",
    "    \"\"\"\n",
    "    day_periods = 24*4\n",
    "    k = 0.0001 # Smoothener for normalizations with potentially 0 divisors\n",
    "    all_features = []\n",
    "    all_meta_features = {}\n",
    "    n_periods = data.shape[0]\n",
    "    # Normal Features\n",
    "    # Current day features\n",
    "    for X in [-7,-6,-5,-4,-3,-2,-1]:\n",
    "        all_features.append(data[7*day_periods+X+3:X+1+n_periods,:,:])\n",
    "    # Previous day features\n",
    "    for D in [-7,-4,-3,-2,-1]:\n",
    "        for X in [-3,-2,-1,0,1,2,3,4]:\n",
    "            all_features.append(data[(7+D)*day_periods + X+3:D*day_periods + X+1+n_periods,:,:])\n",
    "            \n",
    "\n",
    "    # Time period features\n",
    "    day_periods_arr = np.arange(day_periods)/day_periods\n",
    "    \n",
    "    sin_arr = np.sin(2*np.pi*day_periods_arr)\n",
    "    sin_arr = np.tile(sin_arr,int(n_periods/24/4)+int(n_periods%(24*4)>0))\n",
    "    sin_arr = sin_arr[7*day_periods + 3:]\n",
    "    all_features.append(sin_arr[:,None,None]*np.ones((1,1,data.shape[2]))*np.ones((1,data.shape[1],1)))\n",
    "    \n",
    "    cos_arr = np.cos(2*np.pi*day_periods_arr)\n",
    "    cos_arr = np.tile(cos_arr,int(n_periods/24/4)+int(n_periods%(24*4)>0))\n",
    "    cos_arr = cos_arr[7*day_periods + 3:]\n",
    "    all_features.append(cos_arr[:,None,None]*np.ones((1,1,data.shape[2]))*np.ones((1,data.shape[1],1)))\n",
    "    \n",
    "    # Weekday period features\n",
    "    weekday_periods_arr = np.arange(7)/7\n",
    "    \n",
    "    sin_arr = np.sin(2*np.pi*weekday_periods_arr)\n",
    "    sin_arr = np.repeat(sin_arr,day_periods)\n",
    "    sin_arr = np.tile(sin_arr,int(n_periods/24/4/7) + 1)\n",
    "    sin_arr = sin_arr[7*day_periods + 3:-(24*4*7 - n_periods%(24*4*7) - 1)]\n",
    "    all_features.append(sin_arr[:,None,None]*np.ones((1,1,data.shape[2]))*np.ones((1,data.shape[1],1)))\n",
    "    \n",
    "    cos_arr = np.cos(2*np.pi*weekday_periods_arr)\n",
    "    cos_arr = np.repeat(cos_arr,day_periods)\n",
    "    cos_arr = np.tile(cos_arr,int(n_periods/24/4/7) + 1)\n",
    "    cos_arr = cos_arr[7*day_periods + 3:-(24*4*7 - n_periods%(24*4*7) - 1)]\n",
    "    all_features.append(cos_arr[:,None,None]*np.ones((1,1,data.shape[2]))*np.ones((1,data.shape[1],1)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Geospatial features\n",
    "    latt = (np.arange(data.shape[1])[:,None]/data.shape[1]) * np.ones(data.shape[2])[None,:]\n",
    "    long = np.ones(data.shape[1])[:,None] * (np.arange(data.shape[2])[None,:]/data.shape[2])\n",
    "    all_features.extend([\n",
    "        np.ones(n_periods - 7*day_periods-2)[:,None,None]*item[None,:,:]\n",
    "        for item in (latt,long)\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Aggregrated demand features by location\n",
    "    agg_arr = np.sum(data,axis=0,keepdims=True) + k\n",
    "    agg_arr = agg_arr/np.max(agg_arr)\n",
    "    all_features.append(agg_arr*np.ones((n_periods - 7*day_periods-2,1,1)))\n",
    "    all_meta_features['agg_location'] = agg_arr\n",
    "    \n",
    "    \n",
    "    # Aggregrated demand features by time period\n",
    "    full_agg_arr = np.sum(np.sum(data,axis=1,keepdims=True),axis=2,keepdims=True).squeeze()\n",
    "    divisor = full_agg_arr[:(full_agg_arr.shape[0]//96)*96].reshape((-1,96))\n",
    "    divisor = np.max(divisor,axis=0)\n",
    "    all_meta_features['agg_period'] = divisor\n",
    "    divisor = np.tile(divisor, reps=full_agg_arr.shape[0]//96+1)\n",
    "    divisor = divisor[:full_agg_arr.shape[0]]\n",
    "    full_agg_arr /= divisor\n",
    "    # Current day aggregrated demand\n",
    "    for X in [-4,-3,-2,-1]:\n",
    "        agg_arr = full_agg_arr[7*day_periods+X+3:X+1+n_periods,None,None]\n",
    "        all_features.append(agg_arr*np.ones((1,data.shape[1],data.shape[2])))\n",
    "    # Past day aggregrated demand\n",
    "    for D in [-7]:\n",
    "        for X in [-1,0,1]:\n",
    "            agg_arr = full_agg_arr[(7+D)*day_periods + X+3:D*day_periods + X+1+n_periods,None,None]\n",
    "            all_features.append(agg_arr*np.ones((1,data.shape[1],data.shape[2])))\n",
    "    \n",
    "    # Save normalizers\n",
    "    with open(\"Model/all_meta_features.pkl\",\"wb\") as f:\n",
    "        pickle.dump(all_meta_features,f)\n",
    "    return np.stack(all_features,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def format_features_labels(features, targets):\n",
    "    \"\"\"\n",
    "    Formats features and labels into an appropriate shape for training\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : Numpy array\n",
    "        4D Numpy array of demand data, with the shape (n_features, n_feature_periods, n_latt, n_long), where\n",
    "            n_features refers to the number of features for each data sample\n",
    "            n_feature_periods refers to the number of 15-minute periods with features\n",
    "            n_latt refers to the number of lattitude values\n",
    "            n_long refers to the number of longitude values\n",
    "    targets : Numpy array\n",
    "        3D Numpy array of demand data, with the shape (n_feature_periods, n_latt, n_long), where\n",
    "            n_periods refers to the number of 15-minute periods with features\n",
    "            n_latt refers to the number of lattitude values\n",
    "            n_long refers to the number of longitude values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Numpy Array\n",
    "        2D Numpy array of data features, with the shape (n_features, n_samples), where\n",
    "            n_features refers to the number of features for each data sample\n",
    "            n_samples refers to the number of data samples for training\n",
    "    Numpy Array\n",
    "        1D Numpy array of data labels, with the shape (n_samples), where\n",
    "            n_samples refers to the number of data samples for training\n",
    "    \"\"\"\n",
    "    features = features.reshape((features.shape[0],-1)).T\n",
    "    targets = targets.reshape(-1)\n",
    "    \n",
    "    return features,targets\n",
    "\n",
    "\n",
    "def train_lgb(train_X, train_y, params, model=None,max_round=500):\n",
    "    \"\"\"\n",
    "    Trains a LGB with the specified params. If a model is provided, it will continue training that model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_X : Numpy Array\n",
    "        2D Numpy array of data features, with the shape (n_features, n_samples), where\n",
    "            n_features refers to the number of features for each data sample\n",
    "            n_samples refers to the number of data samples for training\n",
    "    train_y : Numpy Array\n",
    "        1D Numpy array of data labels, with the shape (n_samples), where\n",
    "            n_samples refers to the number of data samples for training\n",
    "    params : Dictionary\n",
    "        Dictionary of all parameters for training LightGBM\n",
    "    model : Booster\n",
    "        LightGBM Booster instance representing a trained model, or None to start training from scratch\n",
    "    max_round : int\n",
    "        Maximum number of rounds for training the model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Booster\n",
    "        The trained LightGBM Booster model\n",
    "    \"\"\"\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params, \n",
    "        train_ds, \n",
    "        num_boost_round=max_round,\n",
    "        valid_sets=[train_ds],\n",
    "        verbose_eval=50,\n",
    "        early_stopping_rounds=20,\n",
    "        init_model=model\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_model(demand_data):\n",
    "    \"\"\"\n",
    "    Extracts features from demand_data and fully trains XGBoost model with demand data\n",
    "    Trained model will be saved to /Model folder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    demand_data : Numpy array\n",
    "        3D Numpy array of demand data, with the shape (n_periods, n_latt, n_long), where\n",
    "            n_periods refers to the number of 15-minute periods\n",
    "            n_latt refers to the number of lattitude values\n",
    "            n_long refers to the number of longitude values\n",
    "    \"\"\"\n",
    "    # Extract features for XGBoost\n",
    "    day_periods = 24*4\n",
    "    train_y = demand_data[day_periods*7 + 3:,:,:]\n",
    "    train_X = demand_data[:-1,:,:]\n",
    "    nonzero_demand = np.sum(demand_data,axis=0)\n",
    "    np.save(\"Model/zero_demand.npy\",np.nonzero((nonzero_demand<=0)))\n",
    "    nonzero_demand = np.nonzero((nonzero_demand>0))\n",
    "    \n",
    "    train_X = extract_features(train_X)\n",
    "    train_X = train_X[:,:,nonzero_demand[0],nonzero_demand[1]]\n",
    "    train_y = train_y[:,nonzero_demand[0],nonzero_demand[1]]\n",
    "    train_X,train_y = format_features_labels(train_X,train_y)\n",
    "    del nonzero_demand, demand_data\n",
    "    \n",
    "    \n",
    "    # Train models and save\n",
    "    start = time.time()\n",
    "    # Train LGB Model\n",
    "    lgbm = None\n",
    "    for lr,max_round in zip((0.1,0.05),(1000,500)):\n",
    "        lgb_params['learning_rate'] = lr\n",
    "        lgbm = train_lgb(train_X,train_y,params=lgb_params,model=lgbm,max_round=max_round)\n",
    "    # Train XGBRegressor\n",
    "    gbm = xgb.XGBRegressor(\n",
    "        tree_method='gpu_hist',\n",
    "        max_bin=max_bins,\n",
    "        max_depth=max_depth, \n",
    "        n_estimators=n_estimators, \n",
    "        learning_rate=0.05,\n",
    "        objective='reg:squarederror'\n",
    "    ).fit(train_X, train_y)\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Minutes: %.4f\"%((end-start)/60))\n",
    "\n",
    "\n",
    "    \n",
    "    # Save model to file\n",
    "    pickle.dump(gbm, open(\"Model/xgb_model.model\", \"wb\"))\n",
    "    pickle.dump(lgbm, open(\"Model/lgb_model.model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    demand_data = parse_raw_data()\n",
    "    train_model(demand_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds.\n",
      "[50]\ttraining's rmse: 0.0226132\n",
      "[100]\ttraining's rmse: 0.0221741\n",
      "[150]\ttraining's rmse: 0.0219492\n",
      "[200]\ttraining's rmse: 0.0217981\n",
      "[250]\ttraining's rmse: 0.0216807\n",
      "[300]\ttraining's rmse: 0.0215812\n",
      "[350]\ttraining's rmse: 0.0214924\n",
      "[400]\ttraining's rmse: 0.0214108\n",
      "[450]\ttraining's rmse: 0.0213388\n",
      "[500]\ttraining's rmse: 0.0212744\n",
      "[550]\ttraining's rmse: 0.0212139\n",
      "[600]\ttraining's rmse: 0.0211577\n",
      "[650]\ttraining's rmse: 0.0211009\n",
      "[700]\ttraining's rmse: 0.0210482\n",
      "[750]\ttraining's rmse: 0.0209978\n",
      "[800]\ttraining's rmse: 0.0209483\n",
      "[850]\ttraining's rmse: 0.0209029\n",
      "[900]\ttraining's rmse: 0.0208574\n",
      "[950]\ttraining's rmse: 0.0208148\n",
      "[1000]\ttraining's rmse: 0.0207773\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.0207773\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[1050]\ttraining's rmse: 0.0207578\n",
      "[1100]\ttraining's rmse: 0.0207395\n",
      "[1150]\ttraining's rmse: 0.0207196\n",
      "[1200]\ttraining's rmse: 0.0207013\n",
      "[1250]\ttraining's rmse: 0.0206831\n",
      "[1300]\ttraining's rmse: 0.0206648\n",
      "[1350]\ttraining's rmse: 0.0206454\n",
      "[1400]\ttraining's rmse: 0.0206277\n",
      "[1450]\ttraining's rmse: 0.020611\n",
      "[1500]\ttraining's rmse: 0.0205936\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\ttraining's rmse: 0.0205936\n",
      "Minutes: 12.3271\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
